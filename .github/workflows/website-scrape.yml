name: Website Scraping

on:
  schedule:
    # Runs at 00:00 and 12:00 UTC every day
    - cron: '0 0,12 * * *'
  workflow_dispatch: # Allows manual triggering
    inputs:
      params:
        description: 'parameters to pass to the script'
        required: false
        type: string
        default: ''

jobs:
  scrape-websites:
    runs-on: ubuntu-latest
    environment: Production
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - uses: oven-sh/setup-bun@v2

      - name: Install dependencies
        run: bun install
        # If you don't have a bun.lockb or package.json for dependencies,
        # and your script has no external Bun dependencies other than what's built-in
        # or vendored, you might be able to skip this step.
        # However, it's good practice if your scrapers or common.ts might add some.

      - name: scrape
        env:
          GOOGLE_MAPS_API_KEY: ${{ secrets.GOOGLE_MAPS_API_KEY }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
        run: bun run scripts/scrape-germany.ts ${{ github.event.inputs.params }}

  deduplicate:
    if: ${{ always() }}
    needs: [scrape-websites]
    runs-on: ubuntu-latest
    environment: Production
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - uses: oven-sh/setup-bun@v2

      - name: Install dependencies
        run: bun install
      - name: Remove duplicates
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
          CLOUDINARY_API_KEY: ${{ secrets.CLOUDINARY_API_KEY }}
          CLOUDINARY_CLOUD_NAME: ${{ secrets.CLOUDINARY_CLOUD_NAME }}
          CLOUDINARY_API_SECRET: ${{ secrets.CLOUDINARY_API_SECRET }}
        run: bun run scripts/remove-duplicates.ts ${{ github.event.inputs.params }}
